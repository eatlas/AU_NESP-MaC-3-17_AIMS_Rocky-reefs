"""
Rocky Reef Classification Script

Purpose:
    This script identifies rocky reef habitats in Australian coastal waters using Sentinel-2 satellite imagery.
    It processes satellite imagery tiles across multiple regions (NorthernAU, GBR) to generate rocky reef polygons
    in shapefile format.

Key Algorithms:
    1. Random Forest Classification:
       - Uses a pre-trained Random Forest model to predict rocky reef probabilities
       - Combines false-color (B5, B8, B12) and true-color (B2, B3, B4) Sentinel-2 imagery
       - Outputs probability maps scaled to 8-bit (1-255, with 0 as nodata)
       
    2. Post-processing Pipeline:
       - Clip probability values to [50,130] range and normalize
       - Apply median filter (7x7) to remove noise
       - Perform morphological closing to fill small gaps
       - Double resolution using bilinear interpolation
       - Threshold at 140 to create binary mask
       - Rasterize land mask and remove land pixels
       - Vectorize to polygons with topology-preserving simplification
       - Filter by area (minimum 900 m²)
       
    3. Region & Tile Processing:
       - Processes multiple geographic regions (NorthernAU, GBR)
       - Supports prioritization of specific tiles
       - Handles processing via local loops or SLURM array jobs

Usage:
    python 06-classify-rocky-reefs.py [options]
    
    Options:
        --priority-tiles  : Comma-separated list of tile IDs to process first
        --dataset-path    : Path to the satellite image dataset. This assumes that the dataset
                            has the following structure:
                            <dataset_path>/low_tide_infrared/<region>/*<tile_id>*.tif
                            <dataset_path>/low_tide_true_colour/<region>/*<tile_id>*.tif
                            <dataset_path>/15th_percentile/<region>/*<tile_id>*.tif
        --geotiff-dir     : Temporary working folder for prediction images
        --landmask        : Path to the land mask shapefile
        --regions         : Comma-separated list of regions to process (NorthernAU,GBR)
        --binary-classifier : Use the binary classifier model (default: multi-class classifier)

Example:
    python 06-classify-rocky-reefs.py --priority-tiles "51KVB,51KWB,51LWC,51LXC,51LXD,51LYD,51LZE,52LCK,52LEK,52LGN,53LPH,53LPE,53LPC" --regions NorthernAU
    python 06-classify-rocky-reefs.py --priority-tiles "56KKA,55KFT,55KDV,55LCC,54LYM,56KLU" --regions GBR
 
    Run all regions with no priority tiles:
    python 06-classify-rocky-reefs.py --regions NorthernAU,GBR

    Find out how many tiles are to be processed:
    python 06-classify-rocky-reefs.py --count-only --regions NorthernAU,GBR

    Use the binary classifier model:   
    python 06-classify-rocky-reefs.py --binary-classifier --regions NorthernAU,GBR

    
"""
import os
import sys
import argparse
from datetime import datetime
import numpy as np
import cv2
import rasterio
from rasterio.windows import Window
from rasterio.features import shapes
import joblib
import geopandas as gpd
from shapely.geometry import shape, Polygon, MultiPolygon
from shapely.ops import unary_union
import configparser
import concurrent.futures

MIN_FEATURE_AREA = 1600  # Minimum area in square meters for rocky reef polygons
LOW_TIDE_INFRARED_FOLDER_NAME = 'low_tide_infrared'
LOW_TIDE_TRUE_COLOUR_FOLDER_NAME = 'low_tide_true_colour'
ALL_TIDE_TRUE_COLOUR_FOLDER_NAME = '15th_percentile'

# Directory for saving the generated rocky reef shapefiles for each tile.
MULTI_SHAPEFILE_DIR = 'working/06-multi_w{weight}'
BINARY_SHAPEFILE_DIR = 'working/06-binary_w{weight}'

# Path to the trained Random Forest model and label encoder generated by 04-train-random-forest.py
# This is for the multi-class classifier model option
MULTI_MODEL_PATH = 'working/training-data/random_forest_model_multiclass_w{weight}.pkl'
MULTI_ENCODER_PATH = 'working/training-data/classes_mapping_multiclass_w{weight}.pkl'

# This is for the binary-class classifier model option
BINARY_MODEL_PATH = 'working/training-data/random_forest_model_binary_w{weight}.pkl'
BINARY_ENCODER_PATH = 'working/training-data/classes_mapping_binary_w{weight}.pkl'

def extract_tile_id(filename):
    """
    Extract the tile ID from various filename patterns using regex.
    
    Handles different formats:
    - All tide: AU_AIMS_MARB-S2-comp_p15_TrueColour_56KPU_v2_2015-2024.tif
    - Low tide: AU_AIMS_MARB-S2-comp_low-tide_p30_TrueColour_56KPU.tif
    - Low tide: AU_AIMS_MARB-S2-comp_low-tide_p30_NearInfraredFalseColour_56KPU.tif
    
    Looks for a pattern of underscore followed by two digits and three uppercase letters
    (e.g., "_56KPU") and returns the 5-character tile ID (e.g., "56KPU").
    
    Raises a ValueError if extraction fails.
    """
    import re
    
    # Remove file extension and look for pattern: underscore + two digits + three uppercase letters
    basename = os.path.splitext(filename)[0]
    match = re.search(r'_(\d{2}[A-Z]{3})', basename)
    
    if match:
        tile_id = match.group(1)  # Extract the captured group (without underscore)
        return tile_id
    else:
        # If no match is found, raise an exception
        raise ValueError(f"Invalid tile ID format in filename '{filename}'. "
                         f"Expected a pattern of underscore followed by two digits and three uppercase letters.")

def run_rf_prediction(tile_id, lt_false_path, lt_true_path, at_true_path, rocky_tif_path, rf, rocky_idx):
    """
    Runs the RF prediction for a single tile and saves the 'Rocky reef' probability as a single-band GeoTIFF.
    The prediction is performed block-by-block.
    The probability (0-1) is scaled to 1-255 (with 0 reserved for nodata).
    """
    with rasterio.open(lt_false_path) as lt_false_src, \
            rasterio.open(lt_true_path) as lt_true_src, \
            rasterio.open(at_true_path) as at_true_src:
        if lt_false_src.width != lt_true_src.width or lt_false_src.height != lt_true_src.height:
            print(f"    Dimension mismatch for tile {tile_id}. Skipping prediction.")
            return False

        # Prepare output profile: single band, 8-bit, LZW compression.
        profile = lt_false_src.profile.copy()
        profile.update({
            'dtype': rasterio.uint8,
            'count': 1,
            'compress': 'LZW'
        })

        # Open output file for writing the rocky reef probability.
        with rasterio.open(rocky_tif_path, 'w', **profile) as dst:
            blocks = list(lt_false_src.block_windows(1))
            num_blocks = len(blocks)
            if num_blocks == 0:
                print(f"    No block windows found for tile {tile_id}.")
                return False

            # Parallel-friendly progress reporting - report on separate lines
            block_counter = 0
            last_percent = -10  # Start at -10 so 0% gets reported
            print(f"[{nowstr()}] Tile {tile_id}: Starting prediction (0% complete)")
            
            # Get nodata values for later.
            lt_false_nodata = lt_false_src.nodata
            lt_true_nodata = lt_true_src.nodata
            at_true_nodata = at_true_src.nodata

            for ji, window in blocks:
                block_counter += 1
                current_percent = int((block_counter / num_blocks) * 100)
                
                # Report progress every 10 percent or at 100%
                if current_percent >= last_percent + 10 or current_percent == 100:
                    print(f"[{nowstr()}] Tile {tile_id}: {current_percent}% complete ({block_counter}/{num_blocks} blocks)")
                    last_percent = current_percent

                # Read the block from both images.
                lt_false_window = lt_false_src.read(window=window)  # shape: (3, h, w) for B5, B8, B12
                lt_true_window = lt_true_src.read(window=window)    # shape: (3, h, w) for B2, B3, B4
                at_true_window = at_true_src.read(window=window)    # shape: (3, h, w) for B2, B3, B4

                # Combine into a 9-band array.
                combined = np.concatenate((lt_false_window, lt_true_window, at_true_window), axis=0)
                h, w = combined.shape[1], combined.shape[2]
                features = combined.reshape(9, -1).T  # shape: (h*w, 9)

                # Get class probabilities.
                probabilities = rf.predict_proba(features)  # shape: (num_pixels, n_classes)
                # Extract the probability for "Rocky reef" (using its index).
                rocky_probs = probabilities[:, rocky_idx]  # 1D array

                # Create a nodata mask (if a pixel is nodata in either source, mark it).
                if lt_false_nodata is not None:
                    lt_mask_false = np.all(lt_false_window == lt_false_nodata, axis=0)
                else:
                    lt_mask_false = np.zeros((h, w), dtype=bool)
                if lt_true_nodata is not None:
                    lt_mask_true = np.all(lt_true_window == lt_true_nodata, axis=0)
                else:
                    lt_mask_true = np.zeros((h, w), dtype=bool)
                if at_true_nodata is not None:
                    at_mask_true = np.all(at_true_window == at_true_nodata, axis=0)
                else:
                    at_mask_true = np.zeros((h, w), dtype=bool)
                nodata_mask = lt_mask_false | lt_mask_true | at_mask_true

                # Scale probability from 0-1 to 1-255.
                # For valid pixels: scaled = round(prob * 254) + 1; nodata pixels remain 0.
                scaled = (np.rint(rocky_probs * 254)).astype(np.uint8) + 1
                # Reshape to block.
                scaled = scaled.reshape(h, w)
                scaled[nodata_mask] = 0

                dst.write(scaled, window=window, indexes=1)
            
            print(f"  Tile {tile_id}: Prediction complete")
    return True

def nowstr():
    """
    Returns the current time as a formatted string for printing progress.
    """
    return datetime.now().strftime("%H:%M:%S")

def postprocess_and_polygonize(tile_id, rocky_tif_path, shapefile_path, land_gdf):
    """
    Postprocess the rocky reef probability GeoTIFF:
      - Clip non-zero pixel values to [50, 130] and linearly re-scale that range to [1,255].
      - Apply a 7-pixel median filter using OpenCV.
      - Apply a morphological closing: dilation (with a round kernel of 7x7) then erosion (same kernel) to fill holes.
      - Double the resolution using bilinear interpolation.
      - Apply a threshold of 140 to create a binary mask.
      - Rasterize the land mask and remove land pixels from the binary mask.
      - Convert the masked binary image to polygons (splitting multi-part geometries and simplifying them).
      - Remove polygons with an area less than 900 m².
      - Save the resulting shapefile at shapefile_path.
    """
    import cv2
    import rasterio
    from rasterio.features import shapes, rasterize
    from shapely.geometry import shape
    import geopandas as gpd
    import numpy as np
    from datetime import datetime
    import os

    # Read the original GeoTIFF.
    with rasterio.open(rocky_tif_path) as src:
        img = src.read(1)  # 8-bit image; 0 = nodata.
        transform = src.transform

    # Convert image to float32 for processing.
    img_float = img.astype(np.float32)
    mask = (img_float > 0)

    # Clip valid pixels to brightness [50,130] and normalize to [1,255].
    # This removes some of the weak noise signals that we don't want
    # to merge in with the median filter.
    LEVEL_MIN = 50
    LEVEL_MAX = 130
    img_clipped = img_float.copy()
    img_clipped[mask] = np.clip(img_clipped[mask], LEVEL_MIN, LEVEL_MAX)
    img_norm = img_clipped.copy()
    # Normalize to [1, 255] range. Save 0 for nodata.
    img_norm[mask] = ((img_norm[mask] - LEVEL_MIN) / (LEVEL_MAX - LEVEL_MIN)) * 254 + 1
    img_norm[~mask] = 0
    img_norm = img_norm.astype(np.uint8)

    # Apply a 7x7 median filter. Use a largish kernel to only keep rocky reefs that are at least 50 m across.
    img_median = cv2.medianBlur(img_norm, 7)
    
    # Morphological closing: dilate then erode with elliptical kernels.
    # Use this to fill small holes in the reef polygons.
    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (7, 7))
    img_closed = cv2.dilate(img_median, kernel, iterations=1)
    img_closed = cv2.erode(img_closed, kernel, iterations=1)

    # Double the resolution using bilinear interpolation.
    # This is done to improve the accuracy of polygon conversion step.
    height, width = img_closed.shape
    img_resized = cv2.resize(img_closed, (width*2, height*2), interpolation=cv2.INTER_LINEAR)

    # Apply threshold of 140 to create a binary mask.
    # A value of 140 was chose to give a good balance between false positives and false negatives.
    ret, img_thresh = cv2.threshold(img_resized, 140, 255, cv2.THRESH_BINARY)

    # Update the affine transform since resolution was doubled.
    new_transform = rasterio.Affine(transform.a / 2, transform.b, transform.c,
                                    transform.d, transform.e / 2, transform.f)

    print(f"[{nowstr()}] Tile {tile_id} Rasterizing land mask and applying raster space clipping")

    # Rasterize the land geometries to the same grid as img_thresh.
    # It is faster to perform a rough clipping in raster space than in vector space.
    # Since out the land mask we use is shrunk at little any edge effects in
    # pixel space will the cleaned up in the final land clipping.
    land_mask = rasterize(
        [(geom, 255) for geom in land_gdf.geometry],
        out_shape=img_thresh.shape,
        transform=new_transform,
        fill=0,
        all_touched=True,
        dtype='uint8'
    )

    # Remove rocky reef pixels that fall on land.
    img_thresh[land_mask == 255] = 0

    print(f"[{nowstr()}] Tile {tile_id} Converting raster mask to polygons")

    # Extract polygons from the modified binary image (only for value 255).
    # Apply simplification that is close to 1 pixel in size, this helps remove
    # any remanent stair case steps in the polygons left over from the raster
    # to polygon conversion.
    polys = []
    SIMPLIFY_TOLERANCE = 0.00007  # Approximately 7 m at the equator.
    for geom, val in shapes(img_thresh, transform=new_transform, mask=(img_thresh==255)):
        if val == 255:
            geom_shape = shape(geom)
            # Split multipolygons into individual polygons.
            if geom_shape.geom_type == 'MultiPolygon':
                for poly in geom_shape:
                    polys.append(poly.simplify(SIMPLIFY_TOLERANCE, preserve_topology=True))
            elif geom_shape.geom_type == 'Polygon':
                polys.append(geom_shape.simplify(SIMPLIFY_TOLERANCE, preserve_topology=True))

    if not polys:
        print("  No rocky reef polygons extracted after raster-based clipping.")
        return False

    # Create GeoDataFrame from the extracted polygons (initially in EPSG:4326).
    reef_gdf = gpd.GeoDataFrame(geometry=polys, crs="EPSG:4326")
    reef_gdf['tile_id'] = os.path.basename(shapefile_path)

    # Reproject to a metric CRS for accurate area calculation.
    # Note: Adjust the EPSG code as needed for your study area.
    reef_gdf_metric = reef_gdf.to_crs(epsg=3857)

    # Compute area (in square metres) and filter out features smaller than MIN_FEATURE_AREA m².
    # This is to limit help remove small artefacts.
    reef_gdf_metric = reef_gdf_metric[reef_gdf_metric.area >= MIN_FEATURE_AREA]

    if reef_gdf_metric.empty:
        print(f"[{nowstr()}] Tile {tile_id} No rocky reef polygons remain after area filtering.")
        return False

    # Reproject back to EPSG:4326.
    reef_gdf = reef_gdf_metric.to_crs("EPSG:4326")

    # Save the resulting polygons as a shapefile.
    reef_gdf.to_file(shapefile_path)
    print(f"[{nowstr()}] Tile {tile_id} Shapefile saved to {shapefile_path}")
    return True

def collect_tiles_for_regions(regions, dataset_path, priority_tiles):
    """
    Collects and returns information about tiles to be processed across multiple regions.
    
    Args:
        regions (list): List of region names (e.g., ["NorthernAU", "GBR"])
        dataset_path (str): Base path to the satellite image dataset
        priority_tiles (list): List of tile IDs to prioritize
        
    Returns:
        tuple: Tuple containing:
            - ordered_tile_ids: Array of tile IDs ordered by priority
            - ordered_lt_false_files: Array of low tide false-color files matching tile_ids order
            - ordered_lt_true_files: Array of low tide true-color files matching tile_ids order
            - ordered_at_true_files: Array of all tide true-color files matching tile_ids order
            - ordered_tile_regions: Array of region names matching tile_ids order
            - missing_priority_tiles: List of priority tiles that couldn't be found (empty if none missing)
    """
    # Initialize empty dictionaries to store files from all regions
    lt_false_files = {}
    lt_true_files = {}
    at_true_files = {}
    tile_regions = {}
    missing_priority_tiles = list(priority_tiles) if priority_tiles else []
    
    def collect_geotiff_files(directory, region):
        """Helper function to collect GeoTIFF files from a directory"""
        geotiff_files = {}
        if not os.path.isdir(directory):
            return geotiff_files
            
        for file in os.listdir(directory):
            if file.lower().endswith('.tif'):
                try:
                    tile_id = extract_tile_id(file)
                    geotiff_files[tile_id] = os.path.join(directory, file)
                    # Only record the region if we haven't seen this tile before
                    if tile_id not in tile_regions:
                        tile_regions[tile_id] = region
                except ValueError as e:
                    print(f"Warning: {e}")
        return geotiff_files
    
    # Process each region and collect files
    for region in regions:
        lt_false_dir = os.path.join(dataset_path, LOW_TIDE_INFRARED_FOLDER_NAME, region)
        lt_true_dir = os.path.join(dataset_path, LOW_TIDE_TRUE_COLOUR_FOLDER_NAME, region)
        at_true_dir = os.path.join(dataset_path, ALL_TIDE_TRUE_COLOUR_FOLDER_NAME, region)  
        
        # Collect files for this region
        region_lt_false = collect_geotiff_files(lt_false_dir, region)
        region_lt_true = collect_geotiff_files(lt_true_dir, region)
        region_at_true = collect_geotiff_files(at_true_dir, region)
        
        # Merge with the main dictionaries
        lt_false_files.update(region_lt_false)
        lt_true_files.update(region_lt_true)
        at_true_files.update(region_at_true)
    
    # Find tile IDs that exist in all three image types
    all_tile_ids = set(lt_false_files.keys()) & set(lt_true_files.keys()) & set(at_true_files.keys())
    
    # Update missing_priority_tiles to contain only tiles not found in all three image types
    if priority_tiles:
        missing_priority_tiles = [t for t in priority_tiles if t not in all_tile_ids]
    
    # Order tile IDs: priority tiles first, then others
    priority_ids = [t for t in priority_tiles if t in all_tile_ids]
    other_ids = [t for t in all_tile_ids if t not in set(priority_tiles)]
    ordered_tile_ids = priority_ids + other_ids
    
    # Create ordered arrays based on the tile_ids order
    ordered_lt_false_files = [lt_false_files[tile_id] for tile_id in ordered_tile_ids]
    ordered_lt_true_files = [lt_true_files[tile_id] for tile_id in ordered_tile_ids]
    ordered_at_true_files = [at_true_files[tile_id] for tile_id in ordered_tile_ids]
    ordered_tile_regions = [tile_regions.get(tile_id, "unknown") for tile_id in ordered_tile_ids]
    
    return (
        ordered_tile_ids,
        ordered_lt_false_files,
        ordered_lt_true_files,
        ordered_at_true_files,
        ordered_tile_regions,
        missing_priority_tiles
    )


def process_tile_tile(tile_id, lt_false_file, lt_true_file, at_true_file, geotiff_dir, shapefile_dir, rf, rocky_idx, land_gdf, region):
    """
    Process a single tile: if the output shapefile exists, skip.
    Otherwise, run the RF prediction (if the prediction GeoTIFF doesn't exist) and then
    postprocess it to create a shapefile (clipped with the provided land mask).
    """
    shapefile_path = os.path.join(shapefile_dir, f"RockyReef_{region}_{tile_id}.shp")
    if os.path.exists(shapefile_path):
        print(f"[{nowstr()}] Tile {tile_id} ({region}): shapefile already exists. Skipping tile.")
        return
    
    rocky_tif_path = os.path.join(geotiff_dir, f"Classified-{region}_{tile_id}.tif")

    # Run RF prediction if the prediction GeoTIFF doesn't exist.
    if not os.path.exists(rocky_tif_path):
        success = run_rf_prediction(tile_id, lt_false_file, lt_true_file, at_true_file, rocky_tif_path, rf, rocky_idx)
        if not success:
            print(f"[{nowstr()}] Tile {tile_id} ({region}): RF prediction failed. Skipping tile.")
            return
    else:
        print(f"[{nowstr()}] Tile {tile_id} ({region}): Prediction GeoTIFF already exists, skipping RF prediction.")

    # Postprocess the prediction GeoTIFF and polygonize, clipping with the land mask.
    success = postprocess_and_polygonize(tile_id, rocky_tif_path, shapefile_path, land_gdf)
    if not success:
        print(f"[{nowstr()}] Tile {tile_id} ({region}): Post-processing failed or no polygons extracted.")
    else:
        print(f"[{nowstr()}] Tile {tile_id} ({region}): Processing complete.")


# Add this function at the module level (outside of any other functions)
def process_tile_worker(args):
    """
    Process a single tile in a worker process.
    
    Args:
        args: Tuple containing (
            i: index,
            tile_id: tile identifier,
            lt_false_file: path to low tide false-color file,
            lt_true_file: path to low tide true-color file,
            at_true_file: path to all tide true-color file,
            geotiff_dir: directory for saving GeoTIFF outputs,
            shapefile_dir: directory for saving shapefile outputs,
            rf: trained random forest model,
            rocky_idx: index of Rocky reef class,
            land_gdf: land geometry dataframe,
            region: region name,
            total_files: total number of files to process
        )
    
    Returns:
        1 if processing was successful, 0 otherwise.
    """
    i, tile_id, lt_false_file, lt_true_file, at_true_file, geotiff_dir, shapefile_dir, rf, rocky_idx, land_gdf, region, total_files = args
    
    print(f"\n[{nowstr()}] Processing tile {tile_id} ({i+1}/{total_files})...")
    try:
        process_tile_tile(tile_id, lt_false_file, lt_true_file, at_true_file, 
                        geotiff_dir, shapefile_dir, rf, rocky_idx, land_gdf, region)
        return 1
    except Exception as e:
        print(f"EXCEPTION: Error processing tile {tile_id}: {e}")
        return 0

def main():

    # Read configuration from config.ini
    config = configparser.ConfigParser()
    config.read('config.ini')
    version = config.get('general', 'version')

    parser = argparse.ArgumentParser(
        description="Progressively process each tile: apply RF prediction, post-process, and output a shapefile per tile."
    )
    parser.add_argument(
        '--priority-tiles', 
        type=str, 
        help="Comma-separated list of tile IDs to process first", 
        default=""
    )
    parser.add_argument(
        '--dataset-path',
        type=str,
        help="Path to the satellite image dataset",
        default=r'D:\AU_AIMS_S2-comp'
    )
    parser.add_argument(
        '--geotiff-dir',
        type=str,
        help="Temporary working folder for prediction images",
        default=r'C:\Temp\gis\AU_NESP-MaC-3-17_AIMS_Rocky-reefs'
    )
    parser.add_argument(
        '--landmask',
        type=str,
        help="Path to the cached (processed) land mask shapefile",
        default=f'data/{version}/in/landmask/Coastline-50k_trimmed.shp'
    )
    parser.add_argument(
        '--regions',
        type=str,
        help="Comma-separated list of regions to process (NorthernAU,GBR)",
        default="NorthernAU,GBR"
    )
    parser.add_argument(
        '--count-only',
        action='store_true',
        help="Only count and print the number of tiles to be processed, don't process them"
    )
    parser.add_argument(
        '--binary-classifier',
        action='store_true',
        help="Use the binary classifier model instead of the multi-class model"
    )

    parser.add_argument(
        '--weight',
        type=str,
        help="Weight of the rocky reef class in the model. Must be one of the values calculated in 04-train-random-forest.py",
        choices=["1.0", "1.5", "2.0", "2.5", "3.0"],
        default="2.0",
        default=1
    )

    parser.add_argument(
        '--parallel',
        type=int,
        help="Number of parallel processes to use (default: 1 for sequential processing)",
        default=1
    )


    
    args = parser.parse_args()

    weight = args.weight
    # Set the input path based on the classifier type
    if args.binary_classifier:
        input_path = "working/06-binary"
    else:
        input_path = "working/06-multi"

    # Process priority tiles if provided.
    priority_tiles = []
    if args.priority_tiles:
        priority_tiles = [pt.strip() for pt in args.priority_tiles.split(',') if pt.strip()]

    # Define directories using the provided command line arguments.
    dataset_path = args.dataset_path
    
    # Get regions to process
    regions = [region.strip() for region in args.regions.split(',') if region.strip()]
    if not regions:
        regions = ["NorthernAU", "GBR"]
    
    tile_ids, lt_false_files, lt_true_files, at_true_files, tile_regions, missing_priority_tiles = \
        collect_tiles_for_regions(regions, dataset_path, priority_tiles)

    # If count-only option is specified, just print the counts and exit
    if args.count_only:
        print(f"\nTotal tiles to process: {len(tile_ids)}")
        print("\nUse these values to set your SLURM array parameters.")
        return
    
    print(f"priority_tiles: {missing_priority_tiles}")
    if len(missing_priority_tiles) > 0:
        raise ValueError(f"Error: The following priority tiles were not found: {', '.join(missing_priority_tiles)}")
    
    # Check if there are any tiles to process
    total_files = len(tile_ids)
    if total_files == 0:
        print(f"No processable tiles found.")
        return 0
    print(f"Processing {total_files} tiles across {len(regions)} regions.")    


    geotiff_dir = args.geotiff_dir
    os.makedirs(geotiff_dir, exist_ok=True)
    
    if args.binary_classifier:
        shapefile_dir = BINARY_SHAPEFILE_DIR.format(weight=weight)
    else:
        shapefile_dir = MULTI_SHAPEFILE_DIR.format(weight=weight)
    
    os.makedirs(shapefile_dir, exist_ok=True)

    # Load the cached land mask.
    landmask_path = args.landmask
    if os.path.exists(landmask_path):
        print("Loading cached adjusted land mask...")
        cached_gdf = gpd.read_file(landmask_path)
        # Assume the cached file contains a single geometry.
        land_mask_geom = cached_gdf.geometry.union_all()
    else:
        print(f"Error: Land mask not found at {landmask_path}.")
        print("Please run the land mask processing script first or download the land mask.")
        sys.exit(1)

    print("Converting cached land mask to GeoDataFrame...")
    land_gdf = gpd.GeoDataFrame(geometry=[land_mask_geom], crs="EPSG:4326")

    print("Loading trained model")

    if args.binary_classifier:
        model_path = BINARY_MODEL_PATH.format(weight=weight)
        encoder_path = BINARY_ENCODER_PATH.format(weight=weight)
    else:  
        model_path = MULTI_MODEL_PATH.format(weight=weight)
        encoder_path = MULTI_ENCODER_PATH.format(weight=weight)
    
    rf = joblib.load(model_path)
    le = joblib.load(encoder_path)

    # Determine the index for "Rocky reef" in the model's classes.
    try:
        rocky_idx = list(le.classes_).index("Rocky reef")
    except ValueError:
        print("Error: 'Rocky reef' is not present in the model's classes.")
        sys.exit(1)
    

    
    # Check if SLURM_ARRAY_TASK_ID is set (for SLURM job array processing)
    slurm_task = os.getenv("SLURM_ARRAY_TASK_ID")
    if slurm_task is not None:
        # Existing SLURM handling code
        try:
            idx = int(slurm_task)
        except ValueError:
            print("Error: SLURM_ARRAY_TASK_ID is not a valid integer.")
            return 0
            
        if idx < 0 or idx >= total_files:
            print(f"Error: SLURM_ARRAY_TASK_ID {idx} is out of bounds for region {tile_regions[idx]} (0-{total_files-1}).")
            return 0
            
        tile_id = tile_ids[idx]
        print(f"\n[{nowstr()}] Processing tile {tile_id} (SLURM_ARRAY_TASK_ID={idx})...")
        process_tile_tile(tile_id, lt_false_files[idx], lt_true_files[idx], at_true_files[idx], 
                          geotiff_dir, shapefile_dir, rf, rocky_idx, land_gdf, tile_regions[idx])
        return 1
    else:
        # Check if parallel processing is requested
        num_workers = args.parallel
        if num_workers > 1:
            print(f"Processing {total_files} tiles in parallel using {num_workers} workers")
            processed_count = 0
            
            # Prepare task arguments for each tile
            worker_args = []
            for i in range(total_files):
                worker_args.append((
                    i,                      # Index
                    tile_ids[i],            # Tile ID
                    lt_false_files[i],      # Path to low tide false-color file
                    lt_true_files[i],       # Path to low tide true-color file
                    at_true_files[i],       # Path to all tide true-color file
                    geotiff_dir,            # Directory for GeoTIFF outputs
                    shapefile_dir,          # Directory for shapefile outputs
                    rf,                     # Trained random forest model
                    rocky_idx,              # Index of Rocky reef class
                    land_gdf,               # Land geometry dataframe
                    tile_regions[i],        # Region name
                    total_files             # Total number of files to process
                ))
            
            # Process tiles in parallel
            with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:
                # Map the worker function to the argument list
                results = list(executor.map(process_tile_worker, worker_args))
                processed_count = sum(results)  # Count successful completions
                
            print(f"\nProcessing complete. Successfully processed {processed_count} of {total_files} tiles.")
        else:
            # Existing sequential processing code
            processed_count = 0
            for i, tile_id in enumerate(tile_ids):
                print(f"\n[{nowstr()}] ------ Processing tile {tile_id} ({i+1}/{total_files}) ------")
                process_tile_tile(tile_id, lt_false_files[i], lt_true_files[i], at_true_files[i], 
                                  geotiff_dir, shapefile_dir, rf, rocky_idx, land_gdf, tile_regions[i])
                processed_count += 1
            
            print(f"\nProcessing complete.")

if __name__ == '__main__':
    main()


