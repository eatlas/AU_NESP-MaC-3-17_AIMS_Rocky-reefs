"""
Random Forest Classifier Training for Rocky Reef Identification

This script trains two Random Forest models:
1. A multi-class classifier that distinguishes between all feature types 
   (Cloud, Coral reef, Open water, Rocky reef, Mangrove, Subtidal Rocky reef)
2. A binary classifier that identifies "Rocky reef" vs "Not Rocky reef"

We create both models for comparison purposes to see if separating the into multiple classes
allows the model to better characterise what is not a rocky reef and thus result in fewer
false positives.

The script:
- Loads pre-processed training data with Sentinel-2 spectral bands
- Trains both models on the same feature set
- Evaluates model performance with classification reports
- Saves both models and their class mappings for later use in prediction

Input: CSV file containing training data with extracted pixel values from Sentinel-2 imagery
Output: Trained model files (.pkl) and class mapping files saved to the working directory
"""

import os
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import joblib
import numpy as np

# Directory to save the trained models and class mappings
MODEL_DIR = r'working/training-data'

# Path to the CSV with extracted pixel values and training data
# Generated by the 02-extract-training-data.py script
INPUT_TRAINING_CSV_PATH = r'working/training-data/training-data-with-pixels.csv'

def main():
    
    # Load the data into a DataFrame
    df = pd.read_csv(INPUT_TRAINING_CSV_PATH)
    
    # Define the feature columns and target variable
    lt_false_color_feature_cols = ['S2_LT_B5', 'S2_LT_B8', 'S2_LT_B12']
    lt_true_color_feature_cols = ['S2_LT_B2', 'S2_LT_B3', 'S2_LT_B4']
    at_true_color_feature_cols = ['S2_AT_B2', 'S2_AT_B3', 'S2_AT_B4']
    all_feature_cols = lt_false_color_feature_cols + lt_true_color_feature_cols + at_true_color_feature_cols

    X = df[all_feature_cols].values
    y = df['FeatType'].values
    
    # Encode the categorical target labels into numeric values
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    
    # Find the index of the rocky reef class
    rocky_reef_index = np.where(le.classes_ == 'Rocky reef')[0][0]
    print(f"Rocky reef index: {rocky_reef_index}, Class name: {le.classes_[rocky_reef_index]}")
    
    # Count samples per class
    unique_classes, counts = np.unique(y, return_counts=True)
    class_counts = dict(zip(unique_classes, counts))
    print("Class distribution before balancing:")
    for cls, count in class_counts.items():
        print(f"  {cls}: {count}")
    
    # Split the data into training and testing sets
    X_train, X_test, y_train, y_test = train_test_split(
        X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)
    

    # ------------ Multi-class Classification Model ------------
    # Create class weights for multi-class model, emphasizing Rocky reef
    multi_class_weights = {i: 1.0 for i in range(len(le.classes_))}
    # The weight of 2.0 was obtained to make the precision and recall of the rocky 
    # reef class approximately equal to each other, and to match the 
    # binary classifier.
    multi_class_weights[rocky_reef_index] = 2.0  # Upweight the Rocky reef class
    
    multi_rf = RandomForestClassifier(
        n_estimators=100, 
        random_state=42,
        class_weight=multi_class_weights,  # Add class weights for multi-class model
        max_depth=10  # Add max_depth to prevent overfitting
    )
    multi_rf.fit(X_train, y_train)
    
    # Evaluate the multi-class model on the test set
    y_pred_multi = multi_rf.predict(X_test)
    print("Multi-class Classification Report:")
    print(classification_report(y_test, y_pred_multi, target_names=le.classes_))
    

    # ------------ Binary Classification Model ------------
    # 2. Create binary labels (Rocky reef vs. Not Rocky reef)
    print("\n--- Training Binary Model (Rocky reef vs. Not Rocky reef) ---")
    y_binary = np.zeros_like(y_encoded)
    y_binary[y_encoded == rocky_reef_index] = 1  # 1 for Rocky reef, 0 for everything else
    
    # Split the data for binary classification
    X_train_binary, X_test_binary, y_train_binary, y_test_binary = train_test_split(
        X, y_binary, test_size=0.2, random_state=42, stratify=y_binary)
    
    binary_class_weights = {
        0: 1,
        1: 3    # Increase the weight of the rocky reef, this increased the recall
                # at the expense of precision, i.e. more false positives, but fewer 
                # false negatives. We will need more manual clean up, but will miss
                # fewer rocky reefs.
    }

    # Setting the max_depth to 10 rather than None limits the depth of the trees
    # and thus how well they can fit the training data, helping to prevent overfitting.
    # Each tree in a Random Forest can create up to 2^(depth) leaf nodes, so
    # with a max depth of 10, each tree can create up to 1024 leaf nodes or about
    # 5 samples per leaf node with 5500 samples. Rocky reefs are approximately 20% of
    # the training data, so we can expect about 1100 samples in the rocky reef class.
    # For weight binary weight class of 3 for rocky reefs:
    # Hyperparameter tuning results:
    # n_estimators max_depth Precision  Recall
    # 500          6         0.60       0.80
    # 500          8         0.65       0.78
    # 500          10        0.73       0.76
    # 500          12        0.79       0.73
    # 500          None      0.85       0.65
    # 100          6         0.60       0.81
    # 100          8         0.66       0.77
    # 100          10        0.73       0.75
    # 100          12        0.80       0.75
    # 100          None      0.85       0.65
    # 50           6         0.59       0.80
    # 50           8         0.65       0.77
    # 50           10        0.75       0.76
    # 50           12        0.80       0.72
    # 50           None      0.84       0.65
    # Conclusion: Going from 50 to 100 estimators improved the performance by about 1%,
    # Going from 100 to 500 made no improvement. A max depth of 10 balanced the precision and
    # recall.
    binary_rf = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        class_weight=binary_class_weights,
        max_depth=10, 
        #bootstrap=True,
        #oob_score=True
    )

    binary_rf.fit(X_train_binary, y_train_binary)
    
    # Evaluate binary model without threshold adjustment
    y_pred_binary = binary_rf.predict(X_test_binary)
    
    print("Binary Classification Report:")
    print(classification_report(y_test_binary, y_pred_binary, 
                              target_names=["Not Rocky reef", "Rocky reef"]))
    
    # Save both models and the label encoder to disk
    os.makedirs(MODEL_DIR, exist_ok=True)
    
    # Save multi-class model and encoder
    multi_model_path = os.path.join(MODEL_DIR, 'random_forest_model_multiclass.pkl')
    multi_mapping_path = os.path.join(MODEL_DIR, 'classes_mapping_multiclass.pkl')
    joblib.dump(multi_rf, multi_model_path)
    joblib.dump(le, multi_mapping_path)
    
    # Save binary model
    binary_model_path = os.path.join(MODEL_DIR, 'random_forest_model_binary.pkl')
    joblib.dump(binary_rf, binary_model_path)
    
    # Save binary model class mapping (no threshold)
    binary_mapping_path = os.path.join(MODEL_DIR, 'classes_mapping_binary.pkl')
    # Create a proper LabelEncoder for the binary case
    binary_le = LabelEncoder()
    binary_le.fit(["Not Rocky reef", "Rocky reef"])
    joblib.dump(binary_le, binary_mapping_path)
    
    print(f"Multi-class Random Forest model saved to: {multi_model_path}")
    print(f"Multi-class mapping saved to: {multi_mapping_path}")
    print(f"Binary Rocky reef classifier saved to: {binary_model_path}")
    print(f"Binary classes mapping saved to: {binary_mapping_path}")
    

if __name__ == '__main__':
    main()
